# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hgUCK5bctAKMaA9RLQZrqaIIo495Lv81
"""

import torch
import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score
from scipy.stats import mode
from transformers import BertTokenizer
from transformers import BertForSequenceClassification, Trainer, TrainingArguments
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset
from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support

comments

train_texts, val_texts, train_labels, val_labels = train_test_split(
    comments["comment_text"].tolist(), comments["label"].tolist(), test_size=0.2, random_state=42
)

tokenizer = BertTokenizer.from_pretrained("bert-base-chinese")

def tokenize_function(texts):
    return tokenizer(texts, padding="max_length", truncation=True, max_length=128)

train_encodings = tokenize_function(train_texts)
val_encodings = tokenize_function(val_texts)

train_dataset = Dataset.from_dict({
    "input_ids": train_encodings["input_ids"],
    "attention_mask": train_encodings["attention_mask"],
    "labels": train_labels
})

val_dataset = Dataset.from_dict({
    "input_ids": val_encodings["input_ids"],
    "attention_mask": val_encodings["attention_mask"],
    "labels": val_labels
})

model = BertForSequenceClassification.from_pretrained("bert-base-chinese", num_labels=2)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=2,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model="f1"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

trainer.train()
eval_results = trainer.evaluate()
eval_results

eval_results

preds_output = trainer.predict(val_dataset)
preds = preds_output.predictions.argmax(-1)
true = preds_output.label_ids

val_df = pd.DataFrame({
    "text": val_texts,
    "true_label": true,
    "pred_label": preds
})

fp_df = val_df[(val_df["true_label"] == 0) & (val_df["pred_label"] == 1)]
fn_df = val_df[(val_df["true_label"] == 1) & (val_df["pred_label"] == 0)]

pd.set_option('display.max_colwidth', None)
print("False Positives (predicted misogyny, actually neutral):")
display(fp_df.sample(5, random_state=1))

print("False Negatives (predicted neutral, actually misogyny):")
display(fn_df.sample(5, random_state=1))

#similar with the conclusion with clustering, there seems to be some mislabelled texts to begin with, where the SWSR dataset actually does not provide us with an accurate labelling

tp_df = val_df[(val_df["true_label"] == 1) & (val_df["pred_label"] == 1)]
tn_df = val_df[(val_df["true_label"] == 0) & (val_df["pred_label"] == 0)]

pd.set_option('display.max_colwidth', None)
print("True Positives (predicted misogyny, actually misogyny):")
display(tp_df.sample(5, random_state=1))

print("True Negatives (predicted neutral, actually neutral):")
display(tn_df.sample(5, random_state=1))

#from what can be seen, there are some texts that the model identifies better than the original label. So I will take a better look at the training dataset and see if there are major mistakes
train = pd.DataFrame(train_texts, train_labels).reset_index()
train.columns = ['label', 'text']
train.head(50)

#there are some false positives here, which indicates that the original labelling might not have considered nuances/connotations/context when labelling. There are some keywords that may have indicated that it is misogynistic. Which leads to me to think if it would be more reasonable to find terms as units of misogyny and track it that way? (doing it the lexicon way which the method also proposes)