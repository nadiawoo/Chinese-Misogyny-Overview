# -*- coding: utf-8 -*-
"""Data Acquisition

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uJEiKj8oKGCd9vWydaeBoZQIFAN0ThdR

## Data Acquisition

This process will attempt to get a variety of posts on weibo in terms of topic, time, and individual/organizations/corporates/government.

There is a lack of archives for past Weibo posts, which means it will need to be obtained through API calls. However, from what I know, the only possible method to retreive data of Weibo posts is through their API calls which requires a query to be made (blank querying unfortunately does not work).

In order to generate topics that traces back as much as possible, and as broad as possible, I tried multiple ways, but I ended up with using Selenium to simulate a browser to webscrape the website: https://www.weibotop.cn/2.0/ which contains a small archive of the top 50 "hot topics" in the past. Detailed to the minute, this archive stores the topic name, its popularity, and rank out of 50. Considering that this was only for a more elaborate/broad dataset of posts, the popularity and changes in rank does not matter. So I chose to sample the hot topics every week on Monday.

The website lagged a lot, the changes reflected by the change of dates often does not show up until a few minutes later. So the topic acquisition was often repetitive and length. Moreover, the earliest data it has is in 2019 October, which only provides us with at most 6 years of hot topics to query from.
"""

pip install requests pyquery selenium webdriver-manager

!apt-get update
!apt-get install -y google-chrome-stable
!pip install selenium webdriver-manager

"""Trying Inspurer Github's method.

This unfortunately did not work becase accessing the website required logging in.
"""

import requests
from lxml import etree

# Weibo Hot Search URL
WEIBO_HOT_URL = "https://s.weibo.com/top/summary"

# User-Agent to mimic a real browser request
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0',
    'Cookie': 'SUB = _2A25K1UC3DeRhGeBN6FER9SbFyDqIHXVpq9x_rDV6PUJbktANLRTlkW1NRCE-4IHhCmLgD1lfqX5KQGh-sSL4Iezw'
}

def get_hot_topics():
    """Scrapes Weibo's hot search page and returns a list of trending topics."""
    response = requests.get(WEIBO_HOT_URL, headers=headers)

    if response.status_code == 200:
        html_content = response.text
        print(html_content[:1000])

        html = etree.HTML(response.text)
        # Extract trending topics
        hot_topics = html.xpath("//td[@class='td-02']/a/text()")
        return hot_topics[:10] #just to sample
    else:
        print("Failed to fetch trending topics.")
        return []

if __name__ == "__main__":
    trending_topics = get_hot_topics()
    print("Trending Weibo Topics:", trending_topics)
    #despite there being cookies, this still doesnt work which suggests that alternative methods may be required

response = requests.get(WEIBO_HOT_URL, headers=headers)
print(response.text)
# Page source still shows that clicking into this page leads to the Sina Weibo Visitor's page

"""Here I was trying to grab from the Weibo's Hot Topic page directly through simulating a browser using Selenium. However, I realized that it only has the most recent data and refreshes every minute, so it did not fit my purpose."""

!apt update
!apt install -y wget curl unzip
!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
!dpkg -i google-chrome-stable_current_amd64.deb || true
!apt --fix-broken install -y
!rm google-chrome-stable_current_amd64.deb
!google-chrome --version
#to have a path to google chrome on google colab for selenium to simulate

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By

options = webdriver.ChromeOptions()
options.binary_location = "/usr/bin/google-chrome-stable"
options.add_argument("--headless")
options.add_argument("--disable-gpu")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

# Initialize WebDriver
driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

driver.get("https://s.weibo.com/top/summary")
driver.implicitly_wait(10)  # Wait for JavaScript to load

hot_topics = [element.text for element in driver.find_elements(By.XPATH, "//td[@class='td-02']/a")]

driver.quit()

print("Trending Weibo Topics:", hot_topics)
#this works, so the next step is to complete this with a for loop added with sleep time to prevent bot detections

import time
import random
import pandas as pd
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By

options = webdriver.ChromeOptions()
options.binary_location = "/usr/bin/google-chrome-stable"
options.add_argument("--headless")
options.add_argument("--disable-gpu")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

all_hot_topics = set()
iteration = 0

try:
    while True:
        iteration += 1
        print(f"\n Scraping Weibo Hot Topics - Iteration {iteration} at {datetime.now()}")

        try:
            driver.get("https://s.weibo.com/top/summary")
            driver.implicitly_wait(10)

            hot_topics = [element.text for element in driver.find_elements(By.XPATH, "//td[@class='td-02']/a")]
            all_hot_topics.update(hot_topics)

            # Save periodically every 10 iterations
            if iteration % 20 == 0:
                df = pd.DataFrame({"Trending Topics": list(all_hot_topics)})
                df.to_csv("topics.csv", index=False, encoding="utf-8-sig")
                print(f"Saved {len(all_hot_topics)} unique topics to {csv_filename}")

        except Exception as e:
            print(f"Error encountered: {e}")

        # Wait randomly between 5-20 seconds before the next request
        sleep_time = random.randint(5, 20)
        print(f"Sleeping for {sleep_time} seconds...")
        time.sleep(sleep_time)

except KeyboardInterrupt:
    print("\nStopping the scraper manually...")

    # Final save before exit
    df = pd.DataFrame({"Trending Topics": list(all_hot_topics)})
    df.to_csv("topics.csv", index=False, encoding="utf-8-sig")

    driver.quit()

    #this doesnt work because the hot topics are only the most recent ones where they refresh every 1 minute, to get hsitorical data we would require other datasets.

"""This is the method that I ended up using, where I was able to scrape the archived Hot Topics for querying specific Weibo posts."""

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time
from webdriver_manager.chrome import ChromeDriverManager
from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, ElementClickInterceptedException

# Set up Selenium WebDriver
options = webdriver.ChromeOptions()
options.add_argument("--headless")  # Run in headless mode (no GUI)
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)

# Open Weibo Top Archive website
url = "https://www.weibotop.cn/2.0/"
driver.get(url)

# Wait until the date input field is available
try:
    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "selectDateTime")))
except TimeoutException:
    print("Failed to locate the date input field. Exiting.")
    driver.quit()
    exit()

# Generate weekly dates from 2019-10-28 (first Monday) to 2025-03-10
start_date = pd.to_datetime("2019-10-28")
end_date = pd.to_datetime("2025-03-10")
date_range = pd.date_range(start=start_date, end=end_date, freq='W-MON')  # Every Monday

data = []
save_interval = 10  # Save progress every 10 iterations
csv_filename = "weibo_hot_search_weekly.csv"

for i, date in enumerate(date_range):
    date_str = date.strftime("%m%d%Y")  # MMDDYYYY format

    try:
        # Locate the date input field in every iteration to avoid stale references
        date_input = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "selectDateTime")))

        # Try clicking the element, handling intercepts
        try:
            date_input.click()
        except ElementClickInterceptedException:
            driver.execute_script("arguments[0].click();", date_input)

        time.sleep(1)

        # Enter date manually
        date_input.send_keys(date_str[:2])  # Enter Month (MM)
        time.sleep(0.5)
        date_input.send_keys(date_str[2:4])  # Enter Day (DD)
        time.sleep(0.5)
        date_input.send_keys(date_str[4:])  # Enter Year (YYYY)
        time.sleep(0.5)

        # Click outside to trigger the search
        driver.find_element(By.TAG_NAME, "body").click()
        time.sleep(15)

        # Extract trending topics
        topics = driver.find_elements(By.XPATH, "//h5[@class='mb-1']")
        for topic in topics:
            data.append([date.strftime("%Y-%m-%d"), topic.text])

    except StaleElementReferenceException:
        print(f"⚠️ StaleElementReferenceException encountered on {date.strftime('%Y-%m-%d')}, retrying...")
        continue  # Skip to the next date
    except TimeoutException:
        print(f"⚠️ TimeoutException: No topics found for {date.strftime('%Y-%m-%d')}, skipping...")
        continue

    # Periodic saving every 10 iterations
    if (i + 1) % save_interval == 0:
        df = pd.DataFrame(data, columns=["Date", "Topic"])
        df.to_csv(csv_filename, mode='a', header=not bool(i), index=False)  # Append mode
        print(f"💾 Progress saved at {date.strftime('%Y-%m-%d')} ({i + 1}/{len(date_range)})")
        data = []  # Clear in-memory data to free memory

# Final save for remaining data
if data:
    df = pd.DataFrame(data, columns=["Date", "Topic"])
    df.to_csv(csv_filename, mode='a', header=False, index=False)
    print("✅ Final save completed.")

# Close WebDriver
driver.quit()
print(f"🎉 Data saved to {csv_filename}")

data

data

searches = pd.read_csv("/content/weibo_hot_search_weekly.csv", names=["Date", "Topic"])

topics = searches["Topic"].unique()
 topic_list= topics.tolist()

"""Acquiring Weibo posts from these topics"""

import json
import requests
import time
import os

# Base URL template
url_template = (
    'https://m.weibo.cn/api/container/getIndex'
    '?type=wb'
    '&queryVal={query_val}'
    '&containerid=100103type=2%26q%3D{query_val}'
    '&page={page_num}'
)

def fetch_data(query_val, page_id):
    # Format the URL with the query parameters
    url = url_template.format(query_val=query_val, page_num=page_id)
    print(f"Fetching URL: {url}")

    # Add headers to mimic a browser
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
        'Referer': 'https://m.weibo.cn/',
        'X-Requested-With': 'XMLHttpRequest'
    }

    try:
        resp = requests.get(url, headers=headers)

        if resp.status_code != 200:
            print(f"Error fetching data: HTTP {resp.status_code}")
            return [], False  # Return empty list and False to indicate no more data

        data = resp.json()
        # Check if the response contains 'cards' and it's not empty
        cards = data.get('data', {}).get('cards', [])
        if not cards:
            print("No more cards found.")
            return [], False

        print(f"Fetched {len(cards)} cards from page {page_id}")
        return cards, True  # Return the cards and True to continue fetching
    except Exception as e:
        print(f"Error fetching or parsing data: {e}")
        return [], False

def extract_mblogs(cards):
    mblogs = []
    for card in cards:
        try:
            mblog = card.get('mblog', {})
            blog = {
                'mid': mblog.get('id'),
                'text': mblog.get('text'),
                'userid': str(mblog.get('user', {}).get('id', '')),
                'username': mblog.get('user', {}).get('screen_name', ''),
                'created_at': mblog.get('created_at'),
                'reposts_count': mblog.get('reposts_count', 0),
                'comments_count': mblog.get('comments_count', 0),
                'attitudes_count': mblog.get('attitudes_count', 0)
            }
            mblogs.append(blog)
        except KeyError as e:
            print(f"Missing key in card: {e}")
    return mblogs

def save_backup(mblogs, filename="weibo_data.json"):
    """ Appends new data to an existing JSON file. """
    if os.path.exists(filename):
        with open(filename, 'r', encoding='utf-8') as f:
            try:
                existing_data = json.load(f)
            except json.JSONDecodeError:
                existing_data = []
    else:
        existing_data = []

    existing_data.extend(mblogs)  # Add new data

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(existing_data, f, ensure_ascii=False, indent=4)

    print(f"Saved backup: {len(existing_data)} total posts in {filename}")

def fetch_all_pages(query_val):
    mblogs = []
    page_id = 1  # Start from page 1

    while True:  # Loop until there are no more pages
        cards, has_more_data = fetch_data(query_val, page_id)
        if not has_more_data:
            break  # Stop if no more data is returned

        # Extract mblogs from the cards
        page_mblogs = extract_mblogs(cards)
        mblogs.extend(page_mblogs)

        print(f"Total posts collected so far for '{query_val}': {len(mblogs)}")
        page_id += 1  # Move to the next page

        # Save a backup every 10 pages
        if page_id % 10 == 0:
            save_backup(mblogs)
            mblogs = []  # Clear the list after saving

        # Add a delay to avoid rate-limiting
        time.sleep(2)

    # Final save if there are remaining mblogs
    if mblogs:
        save_backup(mblogs)

def fetch_multiple_queries(query_list):
    for query_val in query_list:
        print(f"\nFetching data for query: '{query_val}'")
        fetch_all_pages(query_val)

# List of query values
query_list = topic_list

# Fetch data for all queries
fetch_multiple_queries(query_list)

"""## Data Preprocessing

With the scraped data, I have to clean the text section to remove the extra hashtag content and the hyperlinks, as well as texts that were not posted by the poster so that it is easier to identify and predict misogyny
"""

import pandas as pd

import json

with open("weibo_data.json", "r", encoding="utf-8") as f:
    try:
        data = json.load(f)
    except json.JSONDecodeError:
        print("JSON is corrupted or incomplete. Trying to fix...")
        f.seek(0)  # Go back to the start of the file
        lines = f.readlines()[:-1]  # Remove the last line (potentially incomplete)

        # Try loading again after removing the last line
        data = json.loads("".join(lines))

df = pd.DataFrame(data)

df.head()
#the text contains a lot of hyperlinks and emojis which takes up tokens and also makes misogyny identification harder

df.shape

df["text"][6] #there is a lot of information that isn't required

import re

def clean_text(text):
    if not text:
        return ""
    # Remove hyperlinks and anchor tags
    text = re.sub(r"http\S+|www\S+|<a.*?>|</a>", "", text)
    # Remove all span classes (surl-text, url-icon, and others)
    text = re.sub(r"<span.*?>.*?</span>", "", text)
    # Remove hashtags (content inside ##)
    text = re.sub(r"#.*?#", "", text)
    # Remove any remaining HTML tags
    text = re.sub(r"<.*?>", "", text)
    # Remove emojis and special characters
    text = re.sub(r"[^\w\s\u4e00-\u9fff]", "", text)
    return text.strip()

# Apply the function to the text column
df["cleaned_text"] = df["text"].apply(clean_text)

# Display cleaned text
df[["text", "cleaned_text"]].head()

df['cleaned_text'][6]

"""taking a sample of weibo dataset for training"""

#saving this for future reference:
df.to_csv("weibo_original_cleaned_df.csv", index=False, encoding="utf-8")

sample_size = 500
df_sample = df.sample(n=sample_size, random_state=42)
df_sample["misogyny_label"] = 0 #creating the column for misogyny label

# Save sample for manual labeling
df_sample.to_csv("weibo_sample_for_labeling.csv", index=False, encoding="utf-8")

"""Simultaneously, because this data covers (or tries to cover) as many topics as possible, it may not provide enough consideration for what is considered misogynistic and what is not. So I wanted to include my previous datasets as well for training."""

feminist = pd.read_csv("/content/feministdiscourse.csv")

feminist.head()
#for this dataset, i have previously ran sentiment analysis, collected similar weibo information, and manually labeled them. -1 is feminist, 0 neutral, and 1 is misogynistic.

feminist_cleaned = feminist.drop(columns=["keyword", "sentiment_score", "snownlp_sentiment"])

feminist_cleaned["cleaned_text"] = feminist_cleaned["text"].apply(clean_text) #remove unecessary symbols and hyperlinks

feminist_cleaned["misogyny_label"] = feminist_cleaned["misogyny_label"].replace(-1, 0) #i'm not going to identify what is and isn't feminist for this project

feminist_cleaned.head()

feminist_cleaned["cleaned_text"][2]

feminist_cleaned.shape #500 rows

feminist_cleaned.to_csv("feminist_sample_for_labeling.csv", index=False, encoding="utf-8") #i went through the sample again to see if this is what i wanted

"""merging the dataset and training the macBERT model"""

feminist_labeled = pd.read_csv("/content/labeled_feminist.csv")
feminist_labeled["mid"] = feminist_labeled["mid"].astype(str).str.split('00%').str[0]

weibo_labeled = pd.read_csv("/content/labeled_weibo.csv")
weibo_labeled[['mid', 'userid']] = weibo_labeled[['mid', 'userid']].astype(str)

feminist_labeled.sample(10)

weibo_labeled.drop(columns=['userid'], inplace=True)

weibo_labeled.shape

mlready = pd.concat([feminist_labeled, weibo_labeled])

mlready.head()

"""From these attempts at fine-tuning the model, there are some conclusions that could be made that points to ideas for the final project.

**In terms of improvements for this model**

One way to go about it is continuing to tune the hyperparameters, but it may not be significantly improving the finetuning of this model. Based on the previous evaluation metrics, the epoches seems optimal between 3-5. With the above attempts, raising the learning rate has made the training and validation loss higher, which may suggest that it isn't helpful pertaining to the currently available data & training set. Thus, increasing batches also may not be helpful considering the small training set, as it increases the possibility of overfitting.


**Real-Life Significance of the fine-tuning** *- was it necessary to finetune this model?*

The fine-tuning increased its accuracy by 0.01%,which is not a big improvement from a simplistic approach to classify every post as non-misogynistic. This imbalanced dataset makes it difficult to evaluate the actual abilities of this fine-tuned language model in identifying misogyny. Moreover, perhaps the limited training sample sizes made it unfit to be used for MacBERT. As a large-scale transformer model, it usually needs a larger training sample to effectively learn discriminatory features.

The improvement itself was rather insignificant considering that there is likely only 2 posts that are misogynistic out of 500. The accuracy, precision, and recall is likely to fluctuate largely and is unfair to evaluate its performance with this dataset.


**Issues pertaining the data**

While fine-tuning may be possible and beneficial, I argue that given this dataset, it is unlikely that accuracy could be raised higher. This Weibo dataset was pulled through identifying the top 50 "hot topics" （热搜）and with those topics as queries for the Weibo API. Because these are topics that are commonly discussed among Weibo users, it limits the covered topics to be news or entertainment related. Many of the samples posts that I manually labeled had discussions of specific Chinese-pop stars and celebrities, government-affiliated organizations, current news, and product advertisements that tags itself with hot topics. This suggests that this technique of data acquisition would only pull commonly accepted topics, flushing out more personal or niche discussions. Hence why this dataset contrasts with what was pulled for me previous project (in the feministdiscourse.csv) which were much more focused on feminism or misogynistic language. With the queries revolved around more public-oriented posts, it is natural that the language is more tame with very implicit biases; such as referring to women in their fields with "women+job title", or cursing with female-organ related terms (although conventional, but still implicitly biased). Considering the nature of these posts, the number of possible misogynistic posts, and the limited training sample, I argue that the problem lies more with the data than it does with the hyperparameters of the language model. There is not a lot of room for both mistakes or improvements of prediction for the language model, as seen as the dummy accuracy itself is as high as 98%.


This project reminds me that for me to get a better dataset, I may need search for other ways to get information. The first method is to conduct further data mining. However, the current dataset with 120,000 data took around 15 hours. Additionally, the archive of hot topics were limited from 2019 October to present, which does not give depth in terms of time.

Another alternative would be looking into news archives or Chinese Twitter/Reddit posts which may an outlook in other aspects that is different from mainland Chinese feminism/language-usage.


**Alternatives/Outlooks for final project**

An alternative is to switch a model, either through using a smaller pre-trained language model or through LLM APIs. A smaller model could improve the performance of classifying misogyny when given a small training set, like the one I created for this objective. There are two ways to use LLM for improvements. 1) Using LLM as a method to label for the training set: by giving a prompt and giving examples or terms, the LLM may be able to contextualize and help identifying a more subtle misogyny that a face-front language model through tokenizing like BERT may not be able to do. 2) Using LLM directly for classification and predicting, which could be a faster way to go about it.

I would also propose that the data acquisition method be changed so both the training set and the full dataset could be larger and covers both personal and official posts.

"""